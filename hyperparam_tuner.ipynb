{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuning Random Forest...\n",
      "Best params: {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_depth': 20}\n",
      "  Accuracy: 0.7327\n",
      "  Precision: 0.7452\n",
      "  Recall: 0.8916\n",
      "  F1-score: 0.8118\n",
      "  ROC-AUC: 0.7730950107429952\n",
      "\n",
      "Tuning Extra Trees...\n",
      "Best params: {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_depth': None}\n",
      "  Accuracy: 0.7313\n",
      "  Precision: 0.7396\n",
      "  Recall: 0.9021\n",
      "  F1-score: 0.8128\n",
      "  ROC-AUC: 0.7759763463714472\n",
      "\n",
      "Tuning XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\xgboost\\core.py:158: UserWarning: [16:36:40] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'subsample': 0.6, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.1}\n",
      "  Accuracy: 0.7392\n",
      "  Precision: 0.7544\n",
      "  Recall: 0.8848\n",
      "  F1-score: 0.8144\n",
      "  ROC-AUC: 0.789176393412947\n",
      "\n",
      "Tuning LightGBM...\n",
      "[LightGBM] [Info] Number of positive: 25866, number of negative: 14134\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001697 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 655\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.646650 -> initscore=0.604346\n",
      "[LightGBM] [Info] Start training from score 0.604346\n",
      "Best params: {'num_leaves': 31, 'n_estimators': 100, 'learning_rate': 0.1}\n",
      "  Accuracy: 0.7355\n",
      "  Precision: 0.7616\n",
      "  Recall: 0.8604\n",
      "  F1-score: 0.8080\n",
      "  ROC-AUC: 0.7848247701945267\n",
      "\n",
      "Tuning CatBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "7 fits failed out of a total of 30.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "7 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\catboost\\core.py\", line 5245, in fit\n",
      "    self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline, use_best_model,\n",
      "  File \"C:\\Users\\Dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\catboost\\core.py\", line 2410, in _fit\n",
      "    self._train(\n",
      "  File \"C:\\Users\\Dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\catboost\\core.py\", line 1790, in _train\n",
      "    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "  File \"_catboost.pyx\", line 5017, in _catboost._CatBoost._train\n",
      "  File \"_catboost.pyx\", line 5066, in _catboost._CatBoost._train\n",
      "_catboost.CatBoostError: catboost/libs/train_lib/dir_helper.cpp:20: Can't create train working dir: catboost_info\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:1107: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan 0.73814994 0.70592496 0.74114996\n",
      " 0.74077496 0.73819991 0.73607489 0.73877494]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'learning_rate': 0.2, 'iterations': 50, 'depth': 6}\n",
      "  Accuracy: 0.7360\n",
      "  Precision: 0.7530\n",
      "  Recall: 0.8806\n",
      "  F1-score: 0.8118\n",
      "  ROC-AUC: 0.7898477458179874\n",
      "\n",
      "Tuning Gradient Boosting...\n",
      "Best params: {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.1}\n",
      "  Accuracy: 0.7318\n",
      "  Precision: 0.7583\n",
      "  Recall: 0.8591\n",
      "  F1-score: 0.8056\n",
      "  ROC-AUC: 0.7845315924068508\n",
      "\n",
      "Tuning Logistic Regression...\n",
      "Best params: {'solver': 'saga', 'penalty': 'l1', 'C': 0.01}\n",
      "  Accuracy: 0.6927\n",
      "  Precision: 0.6984\n",
      "  Recall: 0.9238\n",
      "  F1-score: 0.7954\n",
      "  ROC-AUC: 0.663510703451182\n",
      "\n",
      "Tuning Ridge Classifier...\n",
      "Best params: {'alpha': 0.1}\n",
      "  Accuracy: 0.6937\n",
      "  Precision: 0.6978\n",
      "  Recall: 0.9284\n",
      "  F1-score: 0.7968\n",
      "  ROC-AUC: N/A\n",
      "\n",
      "Tuning SGD Classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 3 is smaller than n_iter=10. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 6 is smaller than n_iter=10. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'loss': 'log_loss', 'alpha': 0.01}\n",
      "  Accuracy: 0.6943\n",
      "  Precision: 0.6961\n",
      "  Recall: 0.9358\n",
      "  F1-score: 0.7984\n",
      "  ROC-AUC: 0.6622808973651901\n",
      "\n",
      "Tuning KNN...\n",
      "Best params: {'weights': 'uniform', 'n_neighbors': 20, 'metric': 'manhattan'}\n",
      "  Accuracy: 0.7184\n",
      "  Precision: 0.7370\n",
      "  Recall: 0.8778\n",
      "  F1-score: 0.8013\n",
      "  ROC-AUC: 0.7272683703993771\n",
      "\n",
      "Tuning GaussianNB...\n",
      "Best params: {'var_smoothing': 1e-09}\n",
      "  Accuracy: 0.6767\n",
      "  Precision: 0.7008\n",
      "  Recall: 0.8727\n",
      "  F1-score: 0.7774\n",
      "  ROC-AUC: 0.6349248296704236\n",
      "\n",
      "Tuning BernoulliNB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 3 is smaller than n_iter=10. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 3 is smaller than n_iter=10. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'alpha': 0.1}\n",
      "  Accuracy: 0.6467\n",
      "  Precision: 0.6467\n",
      "  Recall: 1.0000\n",
      "  F1-score: 0.7854\n",
      "  ROC-AUC: 0.5985993205243141\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"synthetic_data.csv\")\n",
    "categorical_cols = ['Gender', 'Race', 'Education', 'WorkClass', 'Occupation', 'MaritalStatus', 'NativeCountry']\n",
    "numerical_cols = ['Age', 'HoursPerWeek', 'CapitalGain', 'CapitalLoss']\n",
    "X = df.drop(columns=['IncomeClass'])\n",
    "y = df['IncomeClass']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Encode categorical features\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_train[col] = le.fit_transform(X_train[col])\n",
    "    X_test[col] = X_test[col].map(lambda s: le.transform([s])[0] if s in le.classes_ else -1)\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grids = {\n",
    "    \"Random Forest\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"max_depth\": [None, 10, 20],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4],\n",
    "    },\n",
    "    \"Extra Trees\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"max_depth\": [None, 10, 20],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4],\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"max_depth\": [3, 6, 10],\n",
    "        \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "        \"subsample\": [0.6, 0.8, 1.0],\n",
    "    },\n",
    "    \"LightGBM\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"num_leaves\": [31, 50, 100],\n",
    "        \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    },\n",
    "    \"CatBoost\": {\n",
    "        \"iterations\": [50, 100, 200],\n",
    "        \"depth\": [3, 6, 10],\n",
    "        \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "        \"max_depth\": [3, 5, 10],\n",
    "    },\n",
    "    \"Logistic Regression\": {\n",
    "        \"C\": [0.01, 0.1, 1, 10],\n",
    "        \"penalty\": [\"l1\", \"l2\"],\n",
    "        \"solver\": [\"liblinear\", \"saga\"],\n",
    "    },\n",
    "    \"Ridge Classifier\": {\n",
    "        \"alpha\": [0.1, 1, 10],\n",
    "    },\n",
    "    \"SGD Classifier\": {\n",
    "        \"alpha\": [0.0001, 0.001, 0.01],\n",
    "        \"loss\": [\"hinge\", \"log_loss\"],\n",
    "    },\n",
    "    \"KNN\": {\n",
    "        \"n_neighbors\": [3, 5, 10, 20],\n",
    "        \"weights\": [\"uniform\", \"distance\"],\n",
    "        \"metric\": [\"euclidean\", \"manhattan\"],\n",
    "    },\n",
    "    \"GaussianNB\": {\n",
    "        \"var_smoothing\": [1e-9, 1e-8, 1e-7],\n",
    "    },\n",
    "    \"BernoulliNB\": {\n",
    "        \"alpha\": [0.1, 0.5, 1],\n",
    "    }\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "for model_name, param_grid in param_grids.items():\n",
    "    print(f\"\\nTuning {model_name}...\")\n",
    "    model = {\n",
    "        \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "        \"Extra Trees\": ExtraTreesClassifier(random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(eval_metric='logloss', use_label_encoder=False),\n",
    "        \"LightGBM\": LGBMClassifier(),\n",
    "        \"CatBoost\": CatBoostClassifier(verbose=0),\n",
    "        \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "        \"Logistic Regression\": LogisticRegression(),\n",
    "        \"Ridge Classifier\": RidgeClassifier(),\n",
    "        \"SGD Classifier\": SGDClassifier(random_state=42),\n",
    "        \"KNN\": KNeighborsClassifier(),\n",
    "        \"GaussianNB\": GaussianNB(),\n",
    "        \"BernoulliNB\": BernoulliNB(),\n",
    "    }[model_name]\n",
    "    \n",
    "    search = RandomizedSearchCV(model, param_grid, n_iter=10, cv=3, scoring='accuracy', random_state=42, n_jobs=-1)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_model = search.best_estimator_\n",
    "    best_models[model_name] = best_model\n",
    "    \n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_proba = best_model.predict_proba(X_test)[:, 1] if hasattr(best_model, \"predict_proba\") else None\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_proba) if y_proba is not None else \"N/A\"\n",
    "    \n",
    "    print(f\"Best params: {search.best_params_}\")\n",
    "    print(f\"  Accuracy: {acc:.4f}\")\n",
    "    print(f\"  Precision: {prec:.4f}\")\n",
    "    print(f\"  Recall: {rec:.4f}\")\n",
    "    print(f\"  F1-score: {f1:.4f}\")\n",
    "    print(f\"  ROC-AUC: {roc_auc}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
