{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 25866, number of negative: 14134\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001429 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 655\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.646650 -> initscore=0.604346\n",
      "[LightGBM] [Info] Start training from score 0.604346\n",
      "[LightGBM] [Info] Number of positive: 25866, number of negative: 14134\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000329 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1058\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.646650 -> initscore=0.604346\n",
      "[LightGBM] [Info] Start training from score 0.604346\n",
      "[LightGBM] [Info] Number of positive: 25866, number of negative: 14134\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000322 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1058\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.646650 -> initscore=0.604346\n",
      "[LightGBM] [Info] Start training from score 0.604346\n",
      "                      Logistic Regression Ridge Classifier  SGD Classifier  \\\n",
      "Accuracy (Baseline)                0.6927           0.6937          0.6943   \n",
      "Precision (Baseline)               0.6984           0.6978          0.6961   \n",
      "Recall (Baseline)                  0.9238           0.9284          0.9358   \n",
      "F1-score (Baseline)                0.7954           0.7968          0.7984   \n",
      "ROC-AUC (Baseline)                 0.6635              N/A          0.6623   \n",
      "Accuracy (Gaussian)                0.6908           0.6907          0.6915   \n",
      "Precision (Gaussian)               0.6983           0.6982          0.6980   \n",
      "Recall (Gaussian)                  0.9188           0.9188          0.9219   \n",
      "F1-score (Gaussian)                0.7935           0.7935          0.7945   \n",
      "ROC-AUC (Gaussian)                 0.6700              N/A          0.6689   \n",
      "Accuracy (Laplace)                 0.6914           0.6915          0.6920   \n",
      "Precision (Laplace)                0.6986           0.6987          0.6984   \n",
      "Recall (Laplace)                   0.9194           0.9196          0.9218   \n",
      "F1-score (Laplace)                 0.7940            0.794          0.7947   \n",
      "ROC-AUC (Laplace)                  0.6708              N/A          0.6688   \n",
      "\n",
      "                      Random Forest  Extra Trees  XGBoost  LightGBM  CatBoost  \\\n",
      "Accuracy (Baseline)          0.7327       0.7313   0.7358    0.7355    0.7367   \n",
      "Precision (Baseline)         0.7452       0.7396   0.7471    0.7616    0.7580   \n",
      "Recall (Baseline)            0.8916       0.9021   0.8941    0.8604    0.8709   \n",
      "F1-score (Baseline)          0.8118       0.8128   0.8140    0.8080    0.8105   \n",
      "ROC-AUC (Baseline)           0.7731       0.7760   0.7889    0.7848    0.7872   \n",
      "Accuracy (Gaussian)          0.7344       0.7311   0.7359    0.7362    0.7389   \n",
      "Precision (Gaussian)         0.7438       0.7376   0.7486    0.7569    0.7579   \n",
      "Recall (Gaussian)            0.8990       0.9068   0.8908    0.8721    0.8761   \n",
      "F1-score (Gaussian)          0.8141       0.8135   0.8135    0.8105    0.8127   \n",
      "ROC-AUC (Gaussian)           0.7781       0.7775   0.7887    0.7872    0.7885   \n",
      "Accuracy (Laplace)           0.7326       0.7322   0.7387    0.7373    0.7369   \n",
      "Precision (Laplace)          0.7459       0.7398   0.7538    0.7583    0.7551   \n",
      "Recall (Laplace)             0.8896       0.9038   0.8850    0.8715    0.8778   \n",
      "F1-score (Laplace)           0.8114       0.8136   0.8141    0.8110    0.8119   \n",
      "ROC-AUC (Laplace)            0.7801       0.7784   0.7899    0.7876    0.7891   \n",
      "\n",
      "                      Gradient Boosting (GBM)  K-Nearest Neighbors  \\\n",
      "Accuracy (Baseline)                    0.7320               0.7184   \n",
      "Precision (Baseline)                   0.7584               0.7370   \n",
      "Recall (Baseline)                      0.8593               0.8778   \n",
      "F1-score (Baseline)                    0.8057               0.8013   \n",
      "ROC-AUC (Baseline)                     0.7845               0.7273   \n",
      "Accuracy (Gaussian)                    0.7396               0.7055   \n",
      "Precision (Gaussian)                   0.7623               0.7286   \n",
      "Recall (Gaussian)                      0.8681               0.8678   \n",
      "F1-score (Gaussian)                    0.8117               0.7922   \n",
      "ROC-AUC (Gaussian)                     0.7872               0.7176   \n",
      "Accuracy (Laplace)                     0.7382               0.7171   \n",
      "Precision (Laplace)                    0.7645               0.7375   \n",
      "Recall (Laplace)                       0.8601               0.8734   \n",
      "F1-score (Laplace)                     0.8095               0.7997   \n",
      "ROC-AUC (Laplace)                      0.7868               0.7308   \n",
      "\n",
      "                      Gaussian Naive Bayes  Bernoulli Naive Bayes  \n",
      "Accuracy (Baseline)                 0.6767                 0.6467  \n",
      "Precision (Baseline)                0.7008                 0.6467  \n",
      "Recall (Baseline)                   0.8727                 1.0000  \n",
      "F1-score (Baseline)                 0.7774                 0.7854  \n",
      "ROC-AUC (Baseline)                  0.6349                 0.5986  \n",
      "Accuracy (Gaussian)                 0.6908                 0.6467  \n",
      "Precision (Gaussian)                0.6982                 0.6467  \n",
      "Recall (Gaussian)                   0.9193                 1.0000  \n",
      "F1-score (Gaussian)                 0.7936                 0.7854  \n",
      "ROC-AUC (Gaussian)                  0.6439                 0.5817  \n",
      "Accuracy (Laplace)                  0.6878                 0.6467  \n",
      "Precision (Laplace)                 0.6998                 0.6467  \n",
      "Recall (Laplace)                    0.9057                 1.0000  \n",
      "F1-score (Laplace)                  0.7896                 0.7854  \n",
      "ROC-AUC (Laplace)                   0.6454                 0.5840  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "datasets = {\n",
    "    \"Baseline\": \"synthetic_data.csv\",\n",
    "    \"Gaussian\": \"dataset_with_gaussian_noise.csv\",\n",
    "    \"Laplace\": \"dataset_with_laplace_noise.csv\"\n",
    "}\n",
    "\n",
    "categorical_cols = ['Gender', 'Race', 'Education', 'WorkClass', 'Occupation', 'MaritalStatus', 'NativeCountry']\n",
    "numerical_cols = ['Age', 'HoursPerWeek', 'CapitalGain', 'CapitalLoss']\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(solver='saga', penalty='l1', C=0.01, random_state=42),\n",
    "    \"Ridge Classifier\": RidgeClassifier(alpha=0.1),\n",
    "    \"SGD Classifier\": SGDClassifier(loss=\"log_loss\", alpha=0.01, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, min_samples_split=10, min_samples_leaf=2, max_depth=20, random_state=42),\n",
    "    \"Extra Trees\": ExtraTreesClassifier(n_estimators=100, min_samples_split=5, min_samples_leaf=4, max_depth=None, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(subsample=0.6, n_estimators=100, max_depth=3, learning_rate=0.1, eval_metric='logloss', random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(num_leaves=31, n_estimators=100, learning_rate=0.1, random_state=42),\n",
    "    \"CatBoost\": CatBoostClassifier(learning_rate=0.2, iterations=200, depth=3, verbose=0, random_state=42),\n",
    "    \"Gradient Boosting (GBM)\": GradientBoostingClassifier(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(weights='uniform', n_neighbors=20, metric='manhattan'),\n",
    "    \"Gaussian Naive Bayes\": GaussianNB(var_smoothing=1e-09),\n",
    "    \"Bernoulli Naive Bayes\": BernoulliNB(alpha=0.1),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for dataset_name, file_path in datasets.items():\n",
    "    df = pd.read_csv(file_path)\n",
    "    X = df.drop(columns=['IncomeClass'])\n",
    "    y = df['IncomeClass']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        X_train[col] = le.fit_transform(X_train[col])\n",
    "        X_test[col] = X_test[col].map(lambda s: le.transform([s])[0] if s in le.classes_ else -1)\n",
    "        label_encoders[col] = le\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
    "    X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n",
    "\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "        acc = round(accuracy_score(y_test, y_pred), 4)\n",
    "        prec = round(precision_score(y_test, y_pred), 4)\n",
    "        rec = round(recall_score(y_test, y_pred), 4)\n",
    "        f1 = round(f1_score(y_test, y_pred), 4)\n",
    "        roc_auc = round(roc_auc_score(y_test, y_proba), 4) if y_proba is not None else \"N/A\"\n",
    "\n",
    "        if name not in results:\n",
    "            results[name] = {}\n",
    "        \n",
    "        results[name][f\"Accuracy ({dataset_name})\"] = acc\n",
    "        results[name][f\"Precision ({dataset_name})\"] = prec\n",
    "        results[name][f\"Recall ({dataset_name})\"] = rec\n",
    "        results[name][f\"F1-score ({dataset_name})\"] = f1\n",
    "        results[name][f\"ROC-AUC ({dataset_name})\"] = roc_auc\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"model_comparison_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
